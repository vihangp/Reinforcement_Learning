{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\")\n",
    "\n",
    "from libr import plotting    \n",
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"Breakout-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1141ed860>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nITJL+j+3QFvlMGmcOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d7a470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nITJL+j+3QFvlMGmcOgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113d7a6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(env.action_space.n)\n",
    "plt.figure()\n",
    "observation = env.reset()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x114416fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngqSGoSCpYShIahgKkhqGgqTG/wIPZfJqzrI4ggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107a36f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(observation.shape)\n",
    "plt.imshow(observation[34:-16,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 valid actions are available\n",
    "valid_actions = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Resize and grascale image. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(\n",
    "                self.output, [84,84], method = tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
    "            )\n",
    "            self.output = tf.squeeze(self.output)\n",
    "            \n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            sess: A tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "            \n",
    "        Returns: \n",
    "            A processed [84,84,1] state representing grayscale values\n",
    "        \"\"\"\n",
    "        \n",
    "        return sess.run(self.output, {self.input_state: state})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network. \n",
    "    \n",
    "    This network is used for both the Q-Network and the target network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # writes tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            #build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summaries_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "            \n",
    "            \n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build the tensorflow graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Placeholers for our input\n",
    "        # Our input are 4 RGB frames of shape 160(84*), 160(84*) each \n",
    "        \n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype = tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype= tf.int32, name=\"actions\")\n",
    "        \n",
    "        X = tf.to_float(self.X_pl)/255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "        \n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "                X, 32, 8, 4, activation_fn = tf.nn.relu\n",
    "                )\n",
    "        \n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "                conv1, 64, 4, 2, activation_fn = tf.nn.relu\n",
    "        )\n",
    "        \n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "                conv2, 64, 3, 1, activation_fn = tf.nn.relu\n",
    "        )\n",
    "        \n",
    "        # fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(valid_actions))\n",
    "        \n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        # calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "    \n",
    "        # optimizer parameters from the original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        # summaries for tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predict action values.\n",
    "        Args: \n",
    "            sess: Tensorflow session\n",
    "            s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "            \n",
    "        Returns: \n",
    "            Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated\n",
    "            action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, {self.X_pl: s})\n",
    "        \n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "        \n",
    "        Args: \n",
    "            sess: Tensorflow session object\n",
    "            s: state input of shape [batch_size, 4, 160, 160, 3]\n",
    "            a: chosen actions of shape [batch_size]\n",
    "            y: Targets of shape[batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            The calculated loss on the batch. \n",
    "        \"\"\"\n",
    "        \n",
    "        feed_dict = {self.X_pl: s, self.y_pl:y, self.actions_pl:a}\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vihangpatil/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00137653  0.          0.08495298  0.        ]\n",
      " [ 0.00137653  0.          0.08495298  0.        ]]\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis =2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    # test prediction\n",
    "    print(e.predict(sess, observations))\n",
    "    \n",
    "    # test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1,3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph. \n",
    "        Args: \n",
    "            estimator1: Estimator to copy the parameters from\n",
    "            estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        \n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v:v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v:v.name)\n",
    "        \n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy. \n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        \n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon greedy policy based on a given Q function approximator and epsilon.\n",
    "    \n",
    "    Args: \n",
    "        Estimator: an estimator that returns q values for a give state.\n",
    "        nA: Number of actions in the environment.\n",
    "        \n",
    "    Reutrns: \n",
    "        A function that takes the (sess, observation, epsilon) as an argument and \n",
    "        returns the propbabilites for each action in the form of numpy array of length aA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        \n",
    "        A = np.ones(nA, dtype = float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m95\u001b[0m\n\u001b[0;31m    print(\"Populating replay memory...\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_esimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size = 10000,\n",
    "                    replay_memory_init_size = 5000,\n",
    "                    update_target_estimator_every = 10000,\n",
    "                    discount_factor = 0.99,\n",
    "                    epsilon_start = 1.0,\n",
    "                    epsilon_end = 0.1,\n",
    "                    epsilon_decay_steps = 500000,\n",
    "                    batch_size = 32,\n",
    "                    record_video_every = 50):\n",
    "    \n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation. \n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        sess: Tensorflow session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: estimator object used for the targets\n",
    "        state_processor: A state processor object\n",
    "        num_episodes: number of episodes to run for\n",
    "        experiment_dir: directory to save tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sample when \n",
    "        initializing the replay memory\n",
    "        discount_factor: gamma\n",
    "        epsilon_start: chance to sample a random action when taking an action\n",
    "        Epsilon is decayed over time and this is the start value. \n",
    "        epsilon_end: end value of epsilon\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: size of batches to sample from the replay memory\n",
    "        record_video_every: record a video every N episodes\n",
    "        \n",
    "    Returns: \n",
    "        An episodestats object with two numpy arrays for episode_lengths and\n",
    "        episode_rewards.\n",
    "    \"\"\"\n",
    "    Transition = namedtuple(\"Transition\",[\"state\", \"action\", \"next_state\", \"done\"])\n",
    "    \n",
    "    # the replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_esimator, target_estimator)\n",
    "    \n",
    "    # keeps target of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "            episode_lengths = np.zeros(num_episodes),\n",
    "            episode_rewards = np.zeros(num_episodes)\n",
    "            )\n",
    "    \n",
    "    # for 'system/' summaries, useful to check if current process looks healthy\n",
    "    \n",
    "    current_process = psutil.Process()\n",
    "    \n",
    "    # Create directories for checkpoints and summaries\n",
    "    \n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir,\"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if latest_checkpoint:\n",
    "        print(\"loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "        \n",
    "    # get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    # the epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    # the policy we are follwing\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "            q_esimator,\n",
    "        len(valid_actions)\n",
    "    )\n",
    "    \n",
    "    # Populate the replay memory with initial experience\n",
    "   print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(valid_actions[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    # Record videos\n",
    "    # Add env monitor wrapper\n",
    "            \n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every==0, resume=True)\n",
    "            \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            \n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every ==0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\n copied model parameters to target network\")\n",
    "                \n",
    "            # Print out which step we are on, useful for debugging\n",
    "            print(\"\\r step {} ({}) @ epsiode {}/{}, loss: {}\".format(\n",
    "            t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # take a step\n",
    "            \n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p =action_probs)\n",
    "            next_state, reward, done, _= env.step(valid_actions[action])\n",
    "            next_state = state_processor.process(sess,  next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state,2), axis=2)\n",
    "            \n",
    "            # if our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "                \n",
    "            # save the transistion to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "            \n",
    "            # update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            # calculate q values and targets\n",
    "            \n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            # perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_rewards],tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_rewards],tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value = current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value = current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "                episode_lengths = stats.episode_lengths[:i_episode+1],\n",
    "                episode_rewards = stats.episode_rewards[:i_episode+1]\n",
    "        )\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vihangpatil/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating replay memory...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b0b293853096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mepsilon_decay_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     ):\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-27a185ad1013>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_esimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorflow/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5145\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5146\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a global step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope = \"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope = \"target_q\")\n",
    "\n",
    "# state processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(\n",
    "        sess,\n",
    "        env,\n",
    "        q_esimator = q_estimator,\n",
    "        target_estimator = target_estimator,\n",
    "        state_processor = state_processor,\n",
    "        experiment_dir = experiment_dir,\n",
    "        num_episodes = 1000,\n",
    "        replay_memory_size = 5000,\n",
    "        replay_memory_init_size = 5000,\n",
    "        update_target_estimator_every = 50,\n",
    "        epsilon_start = 1.0,\n",
    "        epsilon_end = 0.1,\n",
    "        epsilon_decay_steps = 50000,\n",
    "        discount_factor = 0.99,\n",
    "        batch_size = 32\n",
    "    ):\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
